# GPToughts - Framework personnel d'entraÃ®nement de LLMs

Un framework lÃ©ger mais puissant pour l'entraÃ®nement et le fine-tuning de modÃ¨les de langage (LLMs) comme hobby, avec un accent particulier sur l'optimisation des performances GPU.

## ğŸŒŸ Vue d'ensemble

GPToughts est un projet personnel visant Ã  faciliter l'entraÃ®nement et l'expÃ©rimentation avec des modÃ¨les de langage. Ce framework a Ã©tÃ© conÃ§u pour :

- Servir de plateforme d'apprentissage et d'expÃ©rimentation pour les modÃ¨les LLM
- Maximiser l'utilisation des ressources GPU disponibles
- Offrir une flexibilitÃ© dans le choix des modÃ¨les et des datasets
- Permettre d'implÃ©menter et tester facilement des optimisations de performance

## ğŸ§© CaractÃ©ristiques principales

- **EntraÃ®nement hautement optimisÃ©** : Multiples couches d'optimisations GPU pour maximiser l'utilisation du matÃ©riel
- **Architecture modulaire** : SÃ©paration claire entre les modules de donnÃ©es, d'entraÃ®nement et d'optimisation
- **Support de divers modÃ¨les** : Compatible avec les modÃ¨les de la bibliothÃ¨que Hugging Face Transformers
- **Chargement de donnÃ©es efficace** : ImplÃ©mentation de chargeurs de donnÃ©es asynchrones et optimisÃ©s
- **Optimisations spÃ©cifiques au matÃ©riel** : Configurations adaptÃ©es Ã  diffÃ©rentes architectures GPU (Hopper, Ampere, etc.)
- **EntraÃ®nement distribuÃ©** : Support pour l'entraÃ®nement sur plusieurs GPUs

## ğŸ› ï¸ Structure du projet

```
â”œâ”€â”€ run_train.py              # Point d'entrÃ©e principal pour l'entraÃ®nement
â”œâ”€â”€ run_train_enhanced.py     # Version amÃ©liorÃ©e avec optimisations avancÃ©es
â”œâ”€â”€ gpu_optimization.py       # Optimisations GPU de base
â”œâ”€â”€ gpu_optimization_advanced.py  # Optimisations GPU avancÃ©es
â”œâ”€â”€ gpu_optimization_enhanced.py  # Optimisations GPU encore plus poussÃ©es
â”œâ”€â”€ data/                     # Modules de chargement de donnÃ©es
â”œâ”€â”€ train/                    # Utilitaires et modules d'entraÃ®nement
â”œâ”€â”€ models/                   # Configurations et dÃ©finitions de modÃ¨les
â”œâ”€â”€ checkpoints/              # Sauvegarde des points de contrÃ´le d'entraÃ®nement
â”œâ”€â”€ out/                      # Sorties et logs d'entraÃ®nement
â”œâ”€â”€ scripts d'entraÃ®nement    # Scripts pour lancer diffÃ©rentes configurations
â”‚   â”œâ”€â”€ train_optimized.sh
â”‚   â”œâ”€â”€ train_max_gpu.sh
â”‚   â”œâ”€â”€ train_deepseek_optimized.sh
â”‚   â””â”€â”€ train_deepseek_stable.sh
â””â”€â”€ documentation             # Documentation dÃ©taillÃ©e des fonctionnalitÃ©s
    â”œâ”€â”€ GPU_OPTIMIZATION_README.md
    â”œâ”€â”€ ADVANCED_GPU_OPTIMIZATION.md
    â””â”€â”€ llada.md
```

## ğŸ’» Installation

1. Cloner le dÃ©pÃ´t :
```bash
git clone https://github.com/votre-username/gptoughts.git
cd gptoughts
```

2. Installer les dÃ©pendances :
```bash
pip install -r requirements.txt
```

3. (Optionnel) Pour les optimisations avancÃ©es :
```bash
pip install flash-attn xformers deepspeed
```

## ğŸš€ Utilisation

### EntraÃ®nement de base

```bash
python run_train.py --model_name huggyllama/llama-7b \
                   --block_size 2048 \
                   --batch_size 2 \
                   --learning_rate 1e-5
```

### EntraÃ®nement avec prÃ©cision FP8 (requiert GPU H100/H200)

```bash
python run_train.py --model_type llada \
                   --size medium \
                   --use_fp8 \
                   --batch_size 32 \
                   --block_size 4096 \
                   --learning_rate 1e-5
```

### EntraÃ®nement optimisÃ©

```bash
./train_optimized.sh
```

### EntraÃ®nement avec optimisations maximales

```bash
./train_max_gpu.sh
```

## ğŸ”§ Optimisations GPU

Ce projet met l'accent sur l'optimisation maximale de l'utilisation GPU. Les principales optimisations incluent :

- **Optimisations CUDA avancÃ©es** pour diffÃ©rentes architectures GPU
- **Chargement de donnÃ©es asynchrone** pour rÃ©duire les temps d'attente du GPU
- **Ajustement automatique des paramÃ¨tres d'entraÃ®nement** en fonction du matÃ©riel
- **Optimisations spÃ©cifiques aux opÃ©rations d'attention** (Flash Attention, etc.)
- **Gestion fine de la mÃ©moire** pour rÃ©duire la fragmentation et maximiser l'espace disponible
- **Support de la prÃ©cision FP8** pour les GPUs compatibles (Hopper H100/H200) augmentant significativement la vitesse d'entraÃ®nement

Pour plus de dÃ©tails, consultez les documents `GPU_OPTIMIZATION_README.md` et `ADVANCED_GPU_OPTIMIZATION.md`.

## ğŸ§ª ModÃ¨les supportÃ©s

- ModÃ¨les Llama (Llama 2, Llama 3)
- ModÃ¨les DeepSeek
- Autres modÃ¨les compatibles avec l'interface HuggingFace Transformers

## ğŸ“Š Suivi des expÃ©riences

Le framework intÃ¨gre optionnellement Weights & Biases (wandb) pour le suivi des expÃ©rimentations :
- MÃ©triques d'entraÃ®nement en temps rÃ©el
- Suivi de l'utilisation des ressources GPU
- Comparaison des diffÃ©rentes configurations d'entraÃ®nement

## ğŸ”® Projets futurs

- ImplÃ©mentation de techniques d'optimisation supplÃ©mentaires
- Support pour PEFT (Parameter-Efficient Fine-Tuning)
- IntÃ©gration de techniques d'Ã©valuation automatique de modÃ¨les
- Support pour l'entraÃ®nement hybride CPU/GPU pour les grands modÃ¨les

## ğŸ“ Licence

Ce projet est un projet personnel dÃ©veloppÃ© comme hobby. Merci de respecter la propriÃ©tÃ© intellectuelle.

---

*GPToughts - Parce que l'entraÃ®nement des LLMs devrait Ãªtre accessible Ã  tous les passionnÃ©s d'IA.* 