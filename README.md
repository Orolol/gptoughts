# GPToughts - Framework personnel d'entraÃ®nement de LLMs

Un framework lÃ©ger mais puissant pour l'entraÃ®nement et le fine-tuning de modÃ¨les de langage (LLMs) comme hobby, avec un accent particulier sur l'optimisation des performances GPU.

## ğŸŒŸ Vue d'ensemble

GPToughts est un projet personnel visant Ã  faciliter l'entraÃ®nement et l'expÃ©rimentation avec des modÃ¨les de langage. Ce framework a Ã©tÃ© conÃ§u pour :

- Servir de plateforme d'apprentissage et d'expÃ©rimentation pour les modÃ¨les LLM
- Maximiser l'utilisation des ressources GPU disponibles
- Offrir une flexibilitÃ© dans le choix des modÃ¨les et des datasets
- Permettre d'implÃ©menter et tester facilement des optimisations de performance

## ğŸ§© CaractÃ©ristiques principales

- **EntraÃ®nement hautement optimisÃ©** : Multiples couches d'optimisations GPU pour maximiser l'utilisation du matÃ©riel
- **Architecture modulaire** : SÃ©paration claire entre les modules de donnÃ©es, d'entraÃ®nement et d'optimisation
- **Support de divers modÃ¨les** : Compatible avec les modÃ¨les de la bibliothÃ¨que Hugging Face Transformers
- **Chargement de donnÃ©es efficace** : ImplÃ©mentation de chargeurs de donnÃ©es asynchrones et optimisÃ©s
- **Optimisations spÃ©cifiques au matÃ©riel** : Configurations adaptÃ©es Ã  diffÃ©rentes architectures GPU (Hopper, Ampere, etc.)
- **EntraÃ®nement distribuÃ©** : Support pour l'entraÃ®nement sur plusieurs GPUs

## ğŸ› ï¸ Structure du projet

```
â”œâ”€â”€ run_train.py              # Point d'entrÃ©e principal pour l'entraÃ®nement
â”œâ”€â”€ run_train_enhanced.py     # Version amÃ©liorÃ©e avec optimisations avancÃ©es
â”œâ”€â”€ gpu_optimization.py       # Optimisations GPU de base
â”œâ”€â”€ gpu_optimization_advanced.py  # Optimisations GPU avancÃ©es
â”œâ”€â”€ gpu_optimization_enhanced.py  # Optimisations GPU encore plus poussÃ©es
â”œâ”€â”€ data/                     # Modules de chargement de donnÃ©es
â”œâ”€â”€ train/                    # Utilitaires et modules d'entraÃ®nement
â”œâ”€â”€ models/                   # Configurations et dÃ©finitions de modÃ¨les
â”œâ”€â”€ checkpoints/              # Sauvegarde des points de contrÃ´le d'entraÃ®nement
â”œâ”€â”€ out/                      # Sorties et logs d'entraÃ®nement
â”œâ”€â”€ scripts d'entraÃ®nement    # Scripts pour lancer diffÃ©rentes configurations
â”‚   â”œâ”€â”€ train_optimized.sh
â”‚   â”œâ”€â”€ train_max_gpu.sh
â”‚   â”œâ”€â”€ train_deepseek_optimized.sh
â”‚   â””â”€â”€ train_deepseek_stable.sh
â””â”€â”€ documentation             # Documentation dÃ©taillÃ©e des fonctionnalitÃ©s
    â”œâ”€â”€ GPU_OPTIMIZATION_README.md
    â”œâ”€â”€ ADVANCED_GPU_OPTIMIZATION.md
    â””â”€â”€ llada.md
```

## ï¿½ï¿½ Installation

Pour installer et configurer ce projet :

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/Orolol/gptoughts
cd gptoughts

# Utiliser la branche llada pour les derniÃ¨res fonctionnalitÃ©s
git checkout llada

# Installer les dÃ©pendances
pip install -r requirements.txt

# Configurer le token Hugging Face (nÃ©cessaire pour accÃ©der Ã  certains modÃ¨les)
export HF_TOKEN=your_huggingface_token
```

## ğŸš€ Utilisation

### EntraÃ®nement de base

```bash
python run_train.py --model_name huggyllama/llama-7b \
                   --block_size 2048 \
                   --batch_size 2 \
                   --learning_rate 1e-5
```

### EntraÃ®nement avec prÃ©cision FP8 (requiert GPU H100/H200)

```bash
python run_train.py --model_type llada \
                   --size medium \
                   --use_fp8 \
                   --batch_size 32 \
                   --block_size 4096 \
                   --learning_rate 1e-5
```

### EntraÃ®nement optimisÃ©

```bash
./train_optimized.sh
```

### EntraÃ®nement avec optimisations maximales

```bash
./train_max_gpu.sh
```

## ğŸ”§ Optimisations GPU

Ce projet inclut plusieurs optimisations GPU pour amÃ©liorer les performances d'entraÃ®nement des modÃ¨les LLM. Pour utiliser ces optimisations, exÃ©cutez :

```bash
# Voir toutes les options disponibles
./optimize.sh --help

# Activer toutes les optimisations
./optimize.sh --all

# Activer uniquement certaines optimisations
./optimize.sh --memory --cuda

# Activer les optimisations et lancer l'entraÃ®nement
./optimize.sh --all --train --model llada --size medium
```

Pour plus de dÃ©tails sur les optimisations disponibles, consultez le fichier [OPTIMIZATION_README.md](OPTIMIZATION_README.md).

## ğŸ§ª ModÃ¨les supportÃ©s

- ModÃ¨les Llama (Llama 2, Llama 3)
- ModÃ¨les DeepSeek
- Autres modÃ¨les compatibles avec l'interface HuggingFace Transformers

## ğŸ“Š Suivi des expÃ©riences

Le framework intÃ¨gre optionnellement Weights & Biases (wandb) pour le suivi des expÃ©rimentations :
- MÃ©triques d'entraÃ®nement en temps rÃ©el
- Suivi de l'utilisation des ressources GPU
- Comparaison des diffÃ©rentes configurations d'entraÃ®nement

## ğŸ”® Projets futurs

- ImplÃ©mentation de techniques d'optimisation supplÃ©mentaires
- Support pour PEFT (Parameter-Efficient Fine-Tuning)
- IntÃ©gration de techniques d'Ã©valuation automatique de modÃ¨les
- Support pour l'entraÃ®nement hybride CPU/GPU pour les grands modÃ¨les

## ğŸ“ Licence

Ce projet est un projet personnel dÃ©veloppÃ© comme hobby. Merci de respecter la propriÃ©tÃ© intellectuelle.

---

*GPToughts - Parce que l'entraÃ®nement des LLMs devrait Ãªtre accessible Ã  tous les passionnÃ©s d'IA.* 